\chapter{Related Works}

How content can be Cached, has been researched by different research communities, employing different techniques like small cell Networks(SCNS), this technique employs a network model where various small bass station(SBSs) are able to cache content, this process helps regulate the load on edge servers in a network, providing users with an enjoyable experience and satisfying their different demands overall. Small bass station is placed randomly using a process called Poisson point process. This distributed edge servers provide the content from the internet or by serving them using a locally cached copy. \cite{bacstucache}\cite{bharathlearning}, many other professional user network properties like homophily which assumes ‘birds of the same feather flock together’ to explore how content can be stored in an efficient manner in a complex network like a Content Distribution Networks(CDNs) , using game theory to formulate the caching problem as a many to many matching problem, modelling the small base station and service providers. Attacking the caching problem using a much different point of view. The decision of caching content in local base station is done to reduce the delay experienced by users, this base station use the concept of local popularity of the video content to determine which videos are to be saved, this helps reduce the overall load on the back links of the overall network. \cite{hamidouchemany}. In \cite{sourlasdistributed}, the decision of managing how to effectively cache replicated content across various dedicated storage devices attached to nodes using ICN. Also, in \cite{wangdesign} the problem of caching under constrained storage capacity across routers distributed in the network. Researchers have found that network topology and content popularity are two crucial factors that must be considered to ensure content are effectively cached. \cite{yucongestion} looked at the idea of pushing content to the edge to anticipate network congestion, while \cite{azimdoostfundamental} basically computed the volume of an ad-hoc network can cache. 

In more recent work \cite{li2016pricing}, various game theory concept is utilized for caching popular videos at small cell base stations(SBSs). In other similar works \cite{mangilibandwidth} proposed an approach for ICN that simulate how wireless access point owners can jointly rent their unused storage space to content provider under partial coverage constraints. Both papers instead of providing an efficient content placement solution, mostly targeted a pricing model. 

In his paper \cite{pantazopoulosefficient} defines a conditional “betweenness centrality”, basically using mathematics to map and quantify the interconnection between actors in a network, an idea which is also used here, this metric determine which node in the network will cache content. In \cite{khansaving} a new social-aware metric which is adaptable to dynamic network topology to cache content at vehicle. \cite{bernardinisocially} uses social information in Content Centric Networks (CCNs), this Social-Aware Caching Strategy(SACS) pro-actively cache content produced by influential users in a network, using Page-rank and Eigenvector centrality measure to identify this influential/ prestige users. 

Also in \cite{rossisizing}, centrality based caching approach is used in CCN, where the content to be stored is based upon its centrality.  The authors exploit different centrality like graph eccentricity, degree, stress, closeness and betweenness centrality to miscellaneous decide edge server content should be saved. Its begin proposed that a simple degree centrality based allocation is sufficient to decide which edge server content should be stored also others have proposed that content with high cache hit rate are to be cached at edge servers with higher betweenness centrality. 
Here we use a metric that determine where content can be caches based on the concept of homophily which says, ‘birds of the same feather flock together’. in our case, groups with the same social interest are bound to be interested in the same content, using the social proximity of individuals within a group, we determine where in the network content could be cached, minimizing the overall distance between the cached content and the various individuals within the group.




\section{Introducing the problem}
Today the internet has affected the way people communicate and do business, for different business the effectiveness of this communication is vital for the business to remain afloat, till date various effort continue to be made to improve the overall time over this network. Naturally one would expect that increasing the bandwidth of internet infrastructure will increase the capacity of the server hence reducing the access delay to subscribers, however this is not the case, an increase in the bandwidth of internet infrastructure will not necessarily improve subscribers access time problems. 

Two main occurrences result in bad services quality over the internet, one is the overall lack of management over the internet, the other is the fact that the amount of contents in use over the internet is unimaginable large. The two reasons not only makes access delay a problem over the internet, but  makes the access latency unforeseeable. The approach adopted to address this problem is to have this content moved from their initial origin servers to edge servers closer to the users. Using edge servers to serve use content normally allow for higher content transmission rate and much lower access latency when compared to the when users a served with content from the original server, and the approach of using replicated edge servers to provide this content to users reduces the cost of access even more than having one of such servers as various users are directed to the various different replicated servers, enabling load balancing among this servers. CDN takes this approach by replicating a set of content on the replicated edge servers, sending request for replicated content to the appropriate replicated servers. However deciding how to place content on this 
 replicated edge servers which distribute content to users remain a key challenge in designing an effective CDN, this along side a few others remain one of the key challenges faced by providers like Akamai how provide such servers. Instinctively, it would occur to have this edge servers placed close to the users, but how exactly could this be done

\section{Overview}
CDN are used to get content closer to users, this is done by distributing content to different replicated edge servers placed closer to users. Content provided by this edge servers are content requested by some user at a particular point in time, this requested content stored on the edge server a content with high hit ratio from users.  The overall process ensures that users have access to content very quickly with as less delay as possible and utilize a small amount of bandwidth to access this content.  This particular feature of CDN edge servers make them very coveted by popular content distributors on the internet, providing servers for multiple content distributors using shared resource make this network vital in today's internet. 

\subsection{Distribution System}
CDN distributes the contents from the origin server to replicated edge servers which are much closer to end users. The implication of this is that provide to users little to no access delay and use overall require users to use less network bandwidth.  The internet remain the most commonly used approach to distribute contents to various edge servers, a CDN using this approach maintains an overlay of its distribution tree over the existing Internet infrastructure and disseminates content from the origin server to the replicated edge servers via the tree. Establishing and maintaining this overlay or tree remain a major CDN technical concern. if a generalized approach is used in this overlay, performance issues may arise. 

\subsection{Replicated Edge Server }
Deciding where on the internet to place content, so that clients content access latency as well as the network bandwidth use will be minimized remain the problem, incidentally this two goal have a mutual relationship, such that improving the use of one will lead to improving the use of the other. 

Effective content placment  involves using replicated edge servers all over the internet, various sound approaches have to theorized to model this problem. Many of which is based on the center placement problem.  Because of the complexity of these algorithms, various different heuristics have been proposed which tries to improve upon this algorithms. 

\subsubsection{Theoretical Approaches}
The content placement problem is usually modeled as a metric k-center or metric facility location problem.  This is an NP-hard optimization problem studied in theoretical computer science, it is specified as follows:  Given n cities with specified distances, one wants to build ${k}$ warehouses in different cities and minimize the maximum distance of a city to a warehouse.\cite{jaminplacement}  Another similar type of this problem in computer science is the facility location problem which is concerned with the optimal placement of facilities to minimize transportation costs. \cite{qiuplacement}, problem is also considered NP-hard to solve optimally. Another graph theoretical approach is the k-hierarchically well-separated trees(k-HST)\cite{bartalprobabilistic}\cite{jaminplacement}.

\subsubsection{Heuristic Solutions}
The above theoretical approaches are know to be either computationally expensive or do not take into account the overall features of the network or workload required. Making them unsuitable for practically used in CDN systems\cite{radoslavovtopology}. Some heuristic algorithms have been proposed, this algorithms are designed taking into consideration the network topology or similar observed paths from other existing networks\cite{krishnancache}\cite{qiuplacement}\cite{jaminconstrained}\cite{radoslavovtopology}, this approach provides solutions with much lower computational cost. 

\subsubsection{Greedy Algorithm}
P. Krishman\cite{krishnancache} for the cache location problem proposed a greedy algorithm which was later adapted by Qiu et al.\cite{qiuplacement} for the server placement problem in CDN.

This greedy algorithm after every iteration chooses one site from a set that satisfy certain constrain. Generally the algorithm assumes that client direct their access to the server closest to them, this measure to be like the server that can be reached with the lowest cost.
The process of the algorithm goes as follow: Imagine out of S sites, M potential servers needs to be selected, the first iteration in the algorithm examines all of the S site,
checking to see their suitability to host a server. the cost associated with each site is computed under the assumption that every client request end up at that site, it selects the site that yield the lowest cost, like the lowest bandwidth cost. This process of computing the cost continues in every iteration, each time the selected site yielded the lowest cost in conjunction with the  previously selected site, this pattern continues until the desired number of M servers have been chosen. 

Another algorithm similar to the one described, but various differences was all discussed, this algorithm uses ${l}$-backtracking, this algorithm is called ${l}$-backtracking greedy algorithm, this algorithm unlike the previously described algorithm checks after each iteration all the possible combinations achievable by removing ${l}$ of the already placed servers and replacing them with ${l}$ + 1 new servers. Going by this the previous algorithm is a 0-backtracking greedy algorithm. 

The above greedy algorithm work relatively well, but have a major draw back which is the fact that the greedy placement algorithm requires information of the client location in the network and all pairwise inter-node distances, which in many is not available. 

\subsubsection{Topology-informed Placement Strategy}
Transit Node as this is formally called is a kind of topology-informed placement approach discussed by \cite{jaminconstrained}. its stated as follow, nodes selected as possible host are node with high outward degree, this is done under the assumption that this node have access more nodes with low cost. nodes are selected from highest to lowest number of outward degree. the drawback of this heuristic is the lack of more detailed network topology, it operates under the premise that nodes in the core of the internet transit points will have high degree, and node link corresponnds to AS-level BGP peering.  A slightly better is suggested in\cite{radoslavovtopology}, this approach utilizes the router-level Internet topology, rather than the AS-level topology. i.e this using local area networks associated with a router as a possible site to place this edge server, instead of using AS exchange point as possible site. 

This above describe methods have been found to work equally as well as the greedy placement, and even better measurement conducted has shown that using router-level topology information placement result in a more better performance than all other approach discussed approaches like the different greedy algorithm an the AS-level topology information. But performance issues arise when servers are placed indiscriminately, i.e when high number of servers are placed in the network the performance of this network begin to shrink or decrease. because 
because this capacity of a single host site, have limited capacity, the number of them

Finding the sweet spot where content can be placed in the network remain till this day remains a problem and is considered to be a problem that is not well explored from the various literature available. 	


